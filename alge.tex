\documentclass{article}
\usepackage{amsmath}
\author{a1trl9}
\title{Algebra Review}
\date{}

\setlength\parindent{0pt}
\counterwithin*{equation}{section}

\newcommand{\V}[1]{\boldsymbol{#1}}
\newcommand{\TM}{\bigtriangledown}

\begin{document}
\maketitle

\section{System of Linear Equations}
\textbf{Proposition 1:} for a homogeneous system of linear equations on
\(K\), if the number of equations \(m\) is fewer than the number of
variables \(n\), then it must have a non-trival solution.

\vspace{2mm}
\textbf{Mathematical Induction}

When \(m=1, n>m\), if one coefficient \(a_{1k}=0\), then one
solution is \(x_k=1\) and
all other variables are equal to \(0\). If no
coefficient is equal to \(0\). Then one solution is
\(x_k = -\frac{a_{1k+1}}{a_{k}},
x_{k+1}=1\), all other variables are equal to \(0\).

\vspace{1mm}
Considering when \(m > 1\) while for \(m-1\) the proposition holds.

If all coefficients for \(x_1\) are equal to zero, then one solution is
\(x_1=1, x_2=0, \cdots, x_n=0\).

If not, then change the order of equations so that \(\alpha_{11} \neq 0\).
Now repeat the step \(eq(k) - \frac{a_{11}}{a_{k1}}eq(1)\) where \(eq(t)\)
presents the equation \(t\) and \(k=2,3,\cdots,n\). Finally, the system
of equations are transformed to:
\begin{equation}
\left\{
\begin{split}
    &a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=0 \\
    &a'_{22}x_2 + \cdots + a'_{2n}x_n=0 \\
    &\vdots\\
    &a'_{m2}x_2 + \cdots + a'_{mn}x_n=0 \\
\end{split}
\right.
\end{equation}

Now the second to last equations construct a new homogeneous system
of equations with \(m-1\) equations and \(n-1\) variables:
\begin{equation}
\left\{
\begin{split}
    &a'_{22}x_2 + \cdots + a'_{2n}x_n=0 \\
    &a'_{32}x_2 + \cdots + a'_{3n}x_n=0 \\
    &\vdots\\
    &a'_{m2}x_2 + \cdots + a'_{mn}x_n=0 \\
\end{split}
\right.
\end{equation}

Since \(n>m\), \(n-1>m-1\). And the proposition hold for \(m-1\). There
exists one non-trival solution \(x_2=k_2, \cdots, x_n=k_n\).

Back to (1). Since \(a_{11} \neq 0\), with
\(x_2=k_2, \cdots, x_n=k_n\), there must be one solution \(x_1=k_1\)
letting \(a_{11}k_1+a_{12}k_2+\cdots+a_{1n}k_n=0\).

\section{Vector space \& Matrix \& System of Linear Equations}

\subsection{Vector space}

\textbf{Proposition 2.1.4:} Given two sets of vectors:

\begin{equation}
\begin{split}
    \alpha_1, \alpha_2, \cdots, \alpha_r, \quad\quad (\mathbf{I}) \\
    \beta_1, \beta, \cdots, \beta_s, \quad\quad (\mathbf{II})
\end{split}
\end{equation}

If each vector in \((\mathbf{I})\) can be defined as a linear combination
of \((\mathbf{II})\), and \(r>s\). Then \((\mathbf{I})\) are linearly dependent.

\vspace{2mm}
\textbf{Proof}

As each vector in \((\mathbf{I})\) can be defined as a linear combination
of \((\mathbf{II})\):

\begin{equation}
\begin{split}
&\alpha_1 = x_{11}\beta_1 + x_{12}\beta_2 + \cdots + x_{1s}\beta_s \\
&\alpha_2 = x_{21}\beta_1 + x_{22}\beta_2 + \cdots + x_{2s}\beta_s \\
&\vdots\\
&\alpha_r = x_{r1}\beta_1 + x_{r2}\beta_2 + \cdots + x_{rs}\beta_s \\
\end{split}
\end{equation}

Then given \(k_1, k_2, \cdots, k_s\):

\begin{equation}
\sum_{i=1}^{r}k_i\alpha_i=
\sum_{j=1}^{s}x_{j1}k_j\beta_1
+\sum_{j=1}^{s}x_{j2}k_j\beta_2
+\cdots
+\sum_{j=1}^{s}x_{jr}k_j\beta_r
\end{equation}

In which, since \(r>s\), for:

\begin{equation}
\left
\{
\begin{split}
&x_{11}k_1 + x_{21}k_2 + \cdots + x_{s1}k_s = 0 \\
&x_{12}k_1 + x_{22}k_2 + \cdots + x_{s2}k_s = 0 \\
&\vdots\\
&x_{1r}k_1 + x_{2r}k_2 + \cdots + x_{sr}k_s = 0 \\
\end{split}
\right.
\end{equation}

According to proposition 1 in section 1, there
must be a non-trival solution. That is, there exists \(k_1, k_2, \cdots, k_s\),
in which at least one \(k_n \neq 0\), for \(i = 1, 2, \cdots, r\), \(\sum_{j=1}^s
x_{ji}k_j=0\), in which case \(\sum_{i=1}^rk_i\alpha_i=0\) as well.
. So, \(\alpha_1, \alpha_2,
\cdots, \alpha_r\) are linearly dependent.

\subsection{Matrix operations}

\subsubsection{Operations and rank}

\textbf{Proposition 2.2.4:}: \(r(AB)\leq min(r(A), r(B))\).

\vspace{2mm}
\textit{Lemma 1:}
For two matrics:

\begin{equation}
\begin{split}
    &A=\begin{bmatrix}
    \alpha_1, \alpha_2, \cdots, \alpha_r \\
    \end{bmatrix}\\
    &B=\begin{bmatrix}
    \beta_1, \beta_2, \cdots, \beta_s
    \end{bmatrix}
\end{split}
\end{equation}

if each column vector of \(A\) could be defined as a linear combination
of column vectors in \(B\), then \(r(A)\leq r(B)\).

\vspace{2mm}
\textbf{Proof}

Since each column vector of \(A\) could be defined as a linear combination
of column vectors in \(B\), the maximally linearly independent set
of \(A\): \(\alpha_{i_1}, \alpha_{i_2}, \cdots, \alpha_{i_k}\)
could be defined as a linear combination of
the maximally linearly independent set of \(B\):
\(\beta_{j_1}, \beta_{j_2}, \cdots, \beta_{j_m}\) as well.

Since \(\alpha_{i_1}, \alpha_{i_2}, \cdots, \alpha_{i_k}\) must be
linearly independent, according to proposition 2.1.4,
\(i_k \leq j_m \), that is \(r(A)\leq r(B)\).

\vspace{2mm}
\textbf{Proof for proposition 2.2.4}

Let \(C=AB=\begin{bmatrix}c_1,c_2,\cdots,c_n\end{bmatrix}\). Then
for \(c_i, i = 1, 2, \cdots, n\):

\begin{equation}
\begin{split}
c_i &= \begin{bmatrix}
a_{11}b_{i1}+a_{12}b_{i2}+\cdots+a_{1m}b_{im}\\
a_{21}b_{i1}+a_{22}b_{i2}+\cdots+a_{2m}b_{im}\\
a_{k1}b_{i1}+a_{k2}b_{i2}+\cdots+a_{km}b_{im}\\
\end{bmatrix}\\
&=\alpha_1b_{i1}+\alpha_2b_{i2}+\cdots+\alpha_mb_{im}
\end{split}
\end{equation}

That is, each column vector of \(C\) could be presented as
a combination of column vectors of \(A\). So according
to the lemma, \(r(C)\leq r(A)\). Similarly, \(r(C)\leq r(B)\)
(using transpose). Therefore, \(r(AB)\leq min(r(A), r(B))\).

\subsection{Square matrix}

\textbf{Proposition 2.3.3:} for a \(n\) square matrix \(A\) in the number
field \(K\), the sufficient and necessary condition for \(A\) is
invertable is \(A\) has full rank.

\vspace{2mm}
\textbf{Proof}

\vspace{2mm}
\textit{Sufficiency}

When \(A\) has full rank, there exist elementary matrics
\(P_i, Q_j, i = 1, 2, \cdots, n, j=1,2,\cdots,m\), such that:

\begin{equation}
    P_1P_2\cdots P_nA = E
\end{equation}

And:

\begin{equation}
    AQ_1Q_2\cdots Q_m = E
\end{equation}

Let \(P=P_1P_2\cdots P_n\), and \(Q=Q_1Q_2\cdots Q_m\).

Then:

\begin{equation}
P=PE=PAQ=(PA)Q=EQ=Q
\end{equation}

So \(AP=PA=E\), A is invertable.

\vspace{2mm}

\textit{Necessity}

Since \(A\) is invertable, there exists \(P\), \(AP=E\).
According to proposition 2.2.4, \(r(E)=n\leq min(r(A), r(P))\leq r(A)\).
Therefore \(r(A)=n\) and has full rank.

\vspace{2mm}
\textbf{Proposition 2.3.5}: For a \(n \times n\) square matrix \(\mathbf{X}\),
\(X"X\) is a positive semi-definite matrix.

\vspace{2mm}
\textbf{Proof}
If we present \(X=[X_1, X_2, \cdots, X_n]'\) and
\(X'=[X_1, X_2, \cdots, X_n]\)
It can be shown \(\mathbf{A}=\mathbf{X}'\mathbf{X}\) is symmetric as
\begin{equation}
   a_{ij}=X_i \cdot X_j =a_{ji}
\end{equation}

Where \(a_{kl}\) presents the element located in the \(k\)th row and \(l\)th column
of \(A\).

Then for any non-zero vector \(\V{v}\):
\begin{equation}
\V{v}'A\V{v}=\V{v}'\mathbf{X}'\mathbf{X}\V{v}=(\mathbf{X}\V{v})'\mathbf{X}\V{v}
=||\mathbf{X}\V{v}||^2\geq 0
\end{equation}

\section{Determinant}

\subsection{Definition}

\subsubsection{Linearity and Antisymmetry}

Considering the set consisting of all 
\(n \times n\) matrix \(M_n(K)\) in the number field \(K\) and
a function \(f\) in the domain,
the linearity by row embodies as:

For matrix:

\begin{equation*}
A=
\begin{bmatrix}
    \alpha_1 \\
    \alpha_2 \\
    \vdots \\
    \alpha_n
\end{bmatrix}
\end{equation*}

If for \(i \in [1, n]\):

\begin{equation*}
    \alpha_i=k\alpha+l\beta
\end{equation*}

Then:

\begin{equation}
f(A)=
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix} =
kf(
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha \\
    \vdots \\
    \alpha_n
\end{bmatrix}) + lf(
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \beta \\
    \vdots \\
    \alpha_n
\end{bmatrix}
)
\end{equation}

Similarly, the linearity by column embodies as:

\begin{equation}
\begin{split}
\begin{bmatrix}
    \alpha_1 &
    \cdots &
    \alpha_i &
    \cdots &
    \alpha_n
\end{bmatrix} & =
kf(
\begin{bmatrix}
    \alpha_1 &
    \cdots &
    \alpha &
    \cdots &
    \alpha_n
\end{bmatrix}) \\
& + lf(
\begin{bmatrix}
    \alpha_1 &
    \cdots &
    \beta &
    \cdots &
    \alpha_n
\end{bmatrix}
)
\end{split}
\end{equation}

Moreover, the function \(f\) holding the characteristic above is
said antisymmetric if
\(f(A)=0\) when any two rows/columns of \(A\) are identical. 

\vspace{2mm}
\textbf{Proposition 3.1.1:}

if function \(f\) holds all characteristics above (i.e. linear
by row/column, antisymmetric), then:

\begin{itemize}
    \item Swap two rows/coloums of matrix \(A\) to matrix \(B\),
    \(f(A)=-f(B)\).
    \item Add \(\lambda\alpha_j\) to \(\alpha_i\) to get matrix \(B\),
    \(f(A)=f(B)\).
\end{itemize}

\vspace{2mm}
\textbf{Proof:}

Assuming \(f\) holds linearity by row, then:
\begin{equation}
\begin{split}
f(
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix}) + 
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})
=
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i + \alpha_j \\
    \vdots \\
    \alpha_j + \alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})=0
\end{split}
\end{equation}

Therefore:
\begin{equation}
f(
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix}) = 
-f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})
\end{equation}

Besides:

\begin{equation}
f(
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i + \lambda\alpha_j \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix}) = 
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix}) +
\lambda f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix}) =
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix}) + 0 =
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_j \\
    \vdots \\
    \alpha_n
\end{bmatrix})
\end{equation}

For \(f\) holding the linearity by column, the proof is similar.

\vspace{2mm}
\textbf{Inference 1}

Together with the rule indicated in (1), namely:

\begin{equation}
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \lambda\alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})=
\lambda f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})
\end{equation}

And:

\begin{equation}
f(\begin{bmatrix}
    \alpha_1 &
    \cdots &
    \lambda\alpha_i &
    \cdots &
    \alpha_n
\end{bmatrix})=
\lambda f(\begin{bmatrix}
    \alpha_1 &
    \cdots &
    \alpha_i &
    \cdots &
    \alpha_n
\end{bmatrix})
\end{equation}

These three rules correspond to three types of elementary transformations
by row/column. Therefore, it is obvious that for two functions \(f,g\) holding
antisymmetry and linearity, if \(f(A)=g(A)\) and \(B=AP_1P_2\cdots P_n\)
where \(P_1, P_2, \cdots, P_n\) are all elementary matrics, then
\(f(B)=g(B)\).

\vspace{2mm}
\textbf{Inference 2}

The necessary and sufficient condition for a function \(f\) holding
linearity by row/column to be antisymmetric is for each matrix
\(A\) that is rank deficient, \(f(A)=0\).

\vspace{2mm}
\textbf{Proof:}

\vspace{1mm}
\textit{Sufficient:}

Since for each matrix \(A\) that is rank deficient, \(f(A)=0\),
and obviously if \(A\) has two identical rows/columns, \(A\) is
rank deficient. So \(f(A)=0\) when \(A\) has two identical rows/columns.

\vspace{2mm}
\textit{Necessary:}

Assuming \(f\) holds linearity by row. Then for \(\lambda \neq 0\):

\begin{equation}
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \mathbf{0} \\
    \vdots \\
    \alpha_n
\end{bmatrix} = 
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \lambda \mathbf{0} \\
    \vdots \\
    \alpha_n
\end{bmatrix} = 
\lambda\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \mathbf{0} \\
    \vdots \\
    \alpha_n
\end{bmatrix}
\end{equation}

Therefore:

\begin{equation}
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \mathbf{0} \\
    \vdots \\
    \alpha_n
\end{bmatrix} = 0
\end{equation}

Besides, if \(A\) is rank deficient, with \(\alpha_i = k_1\alpha_1 + \cdots +
k_{i-1}\alpha_{i-1} + k_{i+1}\alpha_{i+1} + \cdots + k_n\alpha_n\),
repeat the step that \(\alpha'_i=\alpha_i - k_j \alpha_{j}\) for
\(k = 1, 2, \cdots, i-1, i+1, \cdots, n\). Finally, \(\alpha'_i=\mathbf{0}\).
Considering \(f\) holds the linearity:

\begin{equation}
f(A)=f(
\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})=
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha'_i \\
    \vdots \\
    \alpha_n
\end{bmatrix})=
f(\begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \mathbf{0} \\
    \vdots \\
    \alpha_n
\end{bmatrix})=0
\end{equation}

For \(f\) holding the linearity by column, the proof is similar.

\subsubsection{Definition}

There are three characteristics that a determinant function holds:

\begin{itemize}
    \item If matrix \(A\) is rank deficient, \(f(A)=0\).
    \item For identity matrix \(E\), \(f(E)=1\).
    \item Linearity by column.
\end{itemize}

\vspace{2mm}
\textbf{Proposition 3.1.2:} The determinant function is unique.

\vspace{2mm}
\textbf{Proof:}

Assuming there are two determinant functions: \(f, g\). 

For matrix \(A\), When \(r(A)<n\), according to the three characteristics,
\(f(A)=g(A)=0\).

When \(r(A)=n\), \(A\) can be presented as \(EP_1P_2\cdots P_m\) where
\(E\) is an identity matrix and \(P_1, P_2, \cdots, P_m\) are elementary
matrics. As \(f(E)=g(E)\), together with \textbf{Inference 1}, 
\(f(A)=g(A)\).

So \(f, g\) are identical.

\vspace{2mm}
\textbf{Proposition 3.1.3:}
For a determinant function \(f\), \(f(A)=f(A')\) where \(A'\) is
the transpose of matrix \(A\).

\vspace{2mm}
\textbf{Proof:}

When \(r(A)<n\), then \(f(A)=f(A')=0\).

When \(r(A)=n\), \(A\) can be presented as \(EP_1P_2\cdots P_m\) where
\(E\) is an identity matrix and \(P_1, P_2, \cdots, P_m\) are elementary
matrics. \(f(A)=(-1)^r\lambda_1\lambda_2\cdots \lambda_s\) where \(r,s\)
are numbers of the type I \& type II elementary matrics (i.e. \(P_i=P_n(j,k)\) or
\(P_i=P_n(\lambda\cdot k)\)) respectively. 
Similarly, \(A'=EP'_mP'_{m-1}\cdots P'_1\). 
\(f(A')=(-1)^{r'}\lambda_1\lambda_2\cdots \lambda_{s'}\).

When \(P_i\) is the
first or second type of elementary, matrics , \(P_i=P'_i\). So \(P_1P_2\cdots P_m\)
and \(P'_mP'_{m-1}\cdots P'_1\) contain the same number of type I and type
II elementary matrics.
When \(P_i\) is the
third type (i.e. \(P_i=P_n(\lambda\cdot j, k)\)), \(P'_i\) is also the
third type, therefore \(f(XP_i)=f(X)\) and \(f(XP'_i)=f(X)\) (\(P_i,
P'_i\) will not change the output of \(f\)).

So \(f(A)=f(EP_1P_2\cdots P_m)=f(EP'_mP'_{m-1}\cdots P'_1)=f(A')\).


\end{document}