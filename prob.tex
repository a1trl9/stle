\documentclass{article}
\author{a1trl9}
\title{Probability Review}
\date{}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{environ}

\newcommand{\V}[1]{\boldsymbol{#1}}
\newcommand{\TM}{\bigtriangledown}

\NewEnviron{mulequation}{%
\begin{equation}\begin{split}
  \BODY
\end{split}\end{equation}
}

\setlength\parindent{0pt}
\counterwithin*{equation}{section}

\begin{document}
\maketitle

\section{Random Variables}

\subsection{Characteristics of Random Variables}

\subsubsection{Expected Value}

For discrete random variable \(X\), expected value is denfined as:

\begin{equation}
\sum_{i=1}^{\infty}x_ip_i
\end{equation}

Where \(p_i\) is the probability for \(X=x_i\).

For continuous random variable \(X\), expected value is defined as:

\begin{equation}
    \int_{\mathbf{R}}f(x)xdx
\end{equation}

Where \(f(x)\) is the probability density function for \(X=x\).

\vspace{2mm}
\textbf{Proposition 1:} For random variables \(X_1, X_2\), no matter if
they are independent, \(E(X_1+X_2)=E(X_1)+E(X_2)\).

\textbf{Proof}

If \(X_1, X_2\) are continuous variables. Then:

\begin{equation}
\begin{split}
    E(X_1 + X_2)&=\int_{-\infty}^{\infty}
    \int_{-\infty}^{\infty}f(x_1, x_2)(x_1+x_2)dx_1dx_2\\
    &=\int_{-\infty}^{\infty}x_1dx_1\int_{-\infty}^{\infty}f(x_1,x_2)dx_2\\
    &+\int_{-\infty}^{\infty}x_2dx_2\int_{-\infty}^{\infty}f(x_1,x_2)dx_1\\
    &=\int_{-\infty}^{\infty}x_1f(x_1)dx_1+\int_{-\infty}^{\infty}x_2f(x_2)dx_2\\
    &=E(X_1)+E(X_2)
\end{split}
\end{equation}

Noting it actually requires rigorous proof to show why the order of
integrations on \(x_1, x_2\) could be interchanged.

\vspace{2mm}
\textbf{Proposition 2:} If \(X_1, X_2\) are two independent random variables,
\(E(X_1X_2)=E(X_1)E(X_2)\).

\vspace{2mm}
\textbf{Proof}

\textit{Only prove when \(X_1, X_2\) are continuous variables.}

Since \(X_1, X_2\) are independent:
\begin{equation}
    f(x_1, x_2)=f(x_1)f(x_2)
\end{equation}

Therefore:

\begin{equation}
\begin{split}
    E(X_1X_2)&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
    x_1x_2f(x_1, x_2)dx_1dx_2\\
    &=\int_{-\infty}^{\infty}f(x_1)dx_1\int_{-\infty}^{\infty}f(x_2)dx_2\\
    &=E(X_1)E(X_2)
\end{split}
\end{equation}

\subsubsection{Variance}

\vspace{2mm}
\textbf{Proposition 3:} If \(X_1, X_2\) are two independent random variables,
\(Var(X_1+X_2)=Var(X_1)+Var(X_2)\).

\vspace{2mm}
\textbf{Proof}
\begin{equation}
\begin{split}
    &Var(X_1+X_2)=E\{[(X_1+X_2)-E(X_1+X_2)]^2\}\\
    &=E[(X_1+X_2)^2]-E^2(X_1+X_2)\\
    &=E(X_1^2)+E(X_2^2)+2E(X_1X_2)-E^2(X_1)-E^2(X_2)-2E(X_1)E(X_2)\\
    &=E(X_1^2)-E^2(X_1)+E(X_2^2)-E^2(X_2)\\
    &=Var(X_1)+Var(X_2)
\end{split}
\end{equation}

\section{Distributions}
\subsection{Discrete Distributions}
\subsubsection{Poisson Distribution}
\textbf{PDF}

\begin{equation}
p(k)=\frac{e^{-\lambda}\lambda^k}{k!}
\end{equation}

Where \(k \in \{0\} \cup \mathbb{N}\).

According to Taylor series (at 0, Maclaurin series):
\begin{equation}
\begin{split}
f(\lambda)&=e^{\lambda}=f(0)+\frac{f'(0)\lambda}{1!}+\frac{f''(0)\lambda^2}{2!}+\cdots\\
&=\frac{\lambda^0}{0!}+\frac{\lambda^1}{1!}+\frac{\lambda^2}{2!}+\cdots\\
&=\sum_{k=0}^\infty\frac{\lambda^k}{k!}
\end{split}
\end{equation}

Therefore:
\begin{equation}
\begin{split}
\sum_{k=0}^{\infty}\frac{e^{-\lambda}\lambda^k}{k!}&=e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}\\
&=e^{-\lambda}e^{\lambda}\\
&=1
\end{split}
\end{equation}

\subsection{Continuous Distributions}
\subsubsection{Exponential Distribution}
\textbf{PDF}

\begin{equation}
p(x)=\lambda e^{-\lambda x}
\end{equation}

It is obvious:
\begin{equation}
\begin{split}
\int_{0}^{\infty}\lambda e^{-\lambda x}&=-e^{-\lambda x}|^{\infty}_0\\
&=-e^{\infty}+e^0=0+1=1
\end{split}
\end{equation}

\subsubsection{Gamma Distribution}
\vspace{2mm}
\textbf{PDF}
\begin{equation}
\int_0^1x^{a-1}(1-x)^{b-1}dx
\end{equation}

\subsubsection{Beta Distribution}
Beta distribution is used as conjugate prior distibution for Bernoulli Distribution.

\vspace{2mm}
\textbf{PDF}
\begin{equation}
p(x)=x^{a-1}(1-x)^{b-1}dx
\end{equation}

\vspace{2mm}
\textbf{Further: Beta function}

Beta function holds an interesting property that:

\begin{equation}
\mathbf{B}(a, b) = \frac{\Gamma{(a)}\Gamma{(b)}}{\Gamma{(a, b)}}
\end{equation}

\vspace{2mm}
\textbf{Proof}

Let \(\Gamma(a)=\int_0^{\infty}e^{-x}x^{a-1}dx\) and \(\Gamma(b)=\int_0^{\infty}e^{-y}y^{b-1}dy\), then:
\begin{equation}
\begin{split}
\Gamma(a)\Gamma(b)&=\int_0^{\infty}e^{-x}x^{a-1}dx \cdot \int_0^{\infty}e^{-y}y^{b-1}dy\\
&=\int_0^{\infty}\int_0^{\infty}e^{-(x+y)}x^{a-1}y^{b-1}dxdy
\end{split}
\end{equation}

Let \(f(u, v): (u, v) \rightarrow (x, y), x=uv, y=u(1-v)\), then:
\begin{equation}
\Gamma{(a)}\Gamma{(b)}=\int_0^{\infty}\int_0^1e^{-u}(uv)^{a-1}(u(1-v))^{b-1}
|D(u, v)|dudv
\end{equation}

Where \(D(\mathbf{J}(u, v))\) is the determinant of Jacobian matrix \(\mathbf{J}\). As:
\begin{equation}
\mathbf{J}=
\begin{bmatrix}
v & 1 - v \\
u & -u
\end{bmatrix}
\end{equation}

\(|D(\mathbf{J}(u, v))|=|-uv-u(1-v)=-u|=u\). So:
\begin{mulequation}
\Gamma{(a)}\Gamma{(b)}=
&\int_0^{\infty}\int_0^1e^{-u}u^{a+b-2}v^{a-1}(1-v)^{b-1}ududv\\
&=\int_0^{\infty}\int_0^1e^{-u}u^{a+b-1}v^{a-1}(1-v)^{b-1}dudv\\
&=\int_0^{\infty}u^{a+b-1}du\int_0^1v^{a-1}(1-v)^{b-1}dv\\
&=\Gamma{(a+b)}\mathbf{B}(a, b)
\end{mulequation}

Here one fact is important:
\(\Gamma(a)=\int_0^{\infty}e^{-x}x^{a-1}dx\) is convergent, which can be proved
as:
\begin{equation}
\begin{split}
\Gamma{(a)}&=\int_0^{k}e^{-x}x^{a-1}dx+\int_k^{\infty}e^{-x}x^{a-1}dx\\
\end{split}
\end{equation}

And it can be shown \(\forall a, \exists k\):
\begin{equation}
\begin{split}
\int_k^{\infty}e^{-x}x^{a-1}dx
<
\int_k^{\infty}e^{\frac{-x}{2}}dx=-2e^{\frac{-x}{2}}|^{\infty}_1=2e^{\frac{-k}{2}}
\end{split}
\end{equation}

and:

\begin{equation}
\int_0^{k}e^{-x}x^{a-1}dx
\end{equation}

is constant.

\section{Linear Regression}
\textbf{Least Square Approach in the form of matrix}
As for:

\begin{equation}
\mathbf{y}=X\beta + \epsilon
\end{equation}

The least square approach tries to minimize:

\begin{equation}
f=||y-X\beta||^2
\end{equation}

Let each partial derivative of \(f\) on \(\beta\) is equal to zero. That is,
for \(\beta_j\):
\begin{equation}
\sum_{i=1}^nx_{ij}(y_i-\mathbf{x}_i\beta)=0
\end{equation}

It could be transformed to:
\begin{equation}
\begin{split}
&\sum_{i=1}^nx_{ij}y_i=\sum_{i=1}^nx_{ij}\mathbf{x}_i\beta\\
&\mathbf{x}'_j\mathbf{y}=\mathbf{x}'_jX\beta
\end{split}
\end{equation}

Now for all partial derivatives:
\begin{equation}
X'\mathbf{y}=X'X\beta
\end{equation}

That is:
\begin{equation}
\beta=(X'X)^{-1}X'\mathbf{y}
\end{equation}

\end{document}
