\documentclass{article}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{amsmath}

\setlength\parindent{0pt}
\lstset{basicstyle=\ttfamily,
language=R,
breaklines=true,
frame=single
}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Sum}[2]{\sum_{#2=0}^n#1_i}

\title{Prob Fragments}
\author{a1trl9}
\date{}

\begin{document}
\maketitle

\section{Concepts}

What is random variables?

What is confidence Interval?

\section{Skills}

\begin{equation}
\begin{split}
&\sum_{i=1}^nX_i^2-(\sum_{i=1}^nX_i)^2/n \\
&=\sum_{i=1}^nX_i^2+(\sum_{i=1}^nX_i)^2/n-2(\sum_{i=1}^nX_i)^2/n\\
&=\sum_{i=1}^n[(X_i-\overline{X})^2 + 2X_i\overline{X}] -2(\sum_{i=1}^nX_i)^2/n\\
&=\sum_{i=1}^n(X_i-\overline{X})^2 + 2n\overline{X}^2-2n\overline{X}^2\\
&=\sum_{i=1}^n(X_i-\overline{X})^2
\end{split}
\end{equation}


\section{Proof}

\textbf{Probability Density Function of \(X_1+X_2\)}

Let \(f(x_1, x_2)\) be the density function of two random variables \(X_1, X_2\). Let random variables \(Y=X_1 + X_2\). We need to know the density function
of \(Y\).

First, consider the function:

\begin{equation}
F(y) = P(Y \leq y)=P(x_1+x_2\leq y)=\iint_Sf(x_1,x_2)dx_1dx_2
\end{equation}

Obviously, $\iint_Sf(x_1,x_2)dx_1dx_2$ could be transformed to:

\begin{equation}
F(y) = \int_{-\infty}^{\infty}dx_1\int_{-\infty}^{y-x_1}f(x_1,x_2)dx_2
\end{equation}

As \(F(y)\) is a cumulative distribution function, it is convergent to 1 in its domain. Also we could prove 

\begin{equation}
f'(y)=\frac{d}{dy}[dx_1\int_{-\infty}^{y-x_1}f(x_1,x_2)dx_2]
\end{equation}

is uniformly
convergent. So the order if derivative and integral could be exchanged.

Therefore, the density function is:

\begin{equation}
\begin{split}
F'(y)&=\int_{-\infty}^{\infty}\frac{d}{dy}[dx_1\int_{-\infty}^{y-x_1}f(x_1,x_2)dx_2]=\int_{-\infty}^{\infty}f(x_1,y-x_1)dx_1\\
&=\int_{-\infty}^{\infty}f(x,y-x)dx
\end{split}
\end{equation}


\vspace{3mm}

\textbf{Standard sample variance (unbiased)}

Why unbiased?

\begin{equation*}
\begin{split}
    E[\sum_{i}^n(x_i-\overline{x})^2]&=\sum_i^nE[(x_i-\overline{x})^2]=\sum_i^n[E(x_i-\frac{1}{n}\sum_{j}^nx_j)^2]\\
    &=n[E({x_i}^2)+\frac{1}{n^2}E(\sum_{j}^nx_j)^2-\frac{2}{n}E(x_i\sum_{j}^nx_j)]\\
    &=n[\sigma^2+\mu^2+\frac{1}{n^2}nE({x_i}^2)+\frac{1}{n^2}\sum_{i}^{n}\sum_{j}^{n-1}E(x_ix_j)\\
    &-\frac{2}{n}E(x_i^2)-\frac{2}{n}(n-1)E(x_ix_j)]\\
    &=n[\sigma^2+\mu^2+\frac{1}{n}(\mu^2+\sigma^2)+\frac{1}{n^2}(n)(n-1)\mu^2\\
    &-\frac{2}{n}(\mu^2+\sigma^2)-\frac{2}{n}(n-1)\mu^2]\\
    &=n[\sigma^2+\mu^2-\frac{1}{n}(\mu^2+\sigma^2)-\frac{1}{n}(n-1)\mu^2]\\
    &=n\frac{n-1}{n}\sigma^2=(n-1)\sigma^2
\end{split}
\end{equation*}

\vspace{3mm}

\textbf{The variance of \(X_1+X_2+\cdots+X_n\)}

If $X_1, X_2, \cdots, X_n$ is i.i.d, then:

\begin{equation}
\Var(X_1+X_2+\cdots+X_n)=\Var(X_1)+\Var(X_2)+\cdots+\Var(X_n)
\end{equation}

\textit{Proof}

We know that for $X_1, X_2, \cdots, X_n$:

\begin{equation}
E(X_1+X_2+\cdots+X_n)=E(X_1)+E(X_2)+\cdots+E(x_n)
\end{equation}

So:

\begin{equation*}
\begin{split}
  \Var(X_1+X_2+\cdots+X_n)&=E{[\sum_i^nX_i-\sum_i^nE(X_i)]^2}\\
  &=E{[\sum_i^n(X_i-E(X_i)]^2}\\
  &=\sum_{i,j}^nE{[X_i-E(X_i)][X_j-E(X_j)]}\\
\end{split}
\end{equation*}

As \(X_i\) and \(X_j\) is independent, \(E(X_i)E(X_j)=E(X_iX_j)\), then for \(i \neq j\):

\begin{equation}
E{[X_i-E(X_i)][X_j-E(X_j)]}=0
\end{equation}

for \(i=j\):

\begin{equation}
E{[X_i-E(X_i)][X_j-E(X_j)]}=E(X_i^2)-E(X_i)^2=\Var(X_i)
\end{equation}

So:

\begin{equation}
\sum_{i,j}^nE\{[X_i-E(X_i)][X_j-E(X_j)]\}=\Var(X_1)+var(X_2)+\cdots+\Var(X_n)
\end{equation}

\vspace{3mm}

\textbf{Variance Estimation}

See \href{https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-5-13}{This paper} for
detailed introduction.

Generally, for data are normally distributed, \(P(2\sigma < X - \mu < 2\sigma) = 0.95\). So when we only know
the range of data \(R\), we could estimate \(\sigma\) as \(\frac{R}{4}\).

When data don not follow a normal distribution, we could use Chbyshev's inequality:

\begin{equation}
P(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}
\end{equation}

Let \(k=3\), then \(P(-3\sigma < X - \mu < 3\sigma)\geq \frac{8}{9}\), we could estimate \(\sigma\) as
\(\frac{\sigma}{6}\).

See \href{https://en.wikipedia.org/wiki/Chebyshev's_inequality}{Wiki} know more about Chebyshev's inequality.

However, if we know the range \(a, b\), mediam \(m\) and size of the sample \(n\), a better estimation is available:

\begin{equation}
\begin{split}
\sigma^2 &= \frac{1}{n-1}[a^2+m^2+b^2+(\frac{n-3}{2})\frac{(a+m)^2+(b+m)^2}{4}\\
&-n(\frac{a+2m+b}{4}+\frac{a-2m+b}{4n})^2]
\end{split}
\end{equation}

See paper for the proof.

\vspace{3mm}

\textbf{Order Statistics}

The probability density function for the order statistics of a sample of size \(n\) drawn from the distribution
of \(X\):
\begin{equation}
f_{X_{k}}(x)=\frac{n!}{(k-1)!(n-k)!}F_X(x)^{k-1}[1-F_X(x)]^{n-k}f_X(x)
\end{equation}

For uniform distribution, the probability of \(X_k\) in the interval \([u, u+du]\) is equal to:

\begin{equation}
\frac{n!}{(k-1)!(n-k)!}u^{k-1}(1-u)^{n-k}du
\end{equation}

which could be derived from (12). And for uniform distribution, the expectation is equal
to \(\frac{k}{n+1}\).

\vspace{3mm}
\textbf{Binomal Distribution}

How to calculate mean for \(X\stackrel{d}{=} B(n, p)\)?
\begin{equation*}
\begin{split}
E(X)&=\sum_{k=0}^n{n \choose k} kp^k(1-p)^{n-k}\\
&=np\sum_{k=0}^n\frac{(n-1)!}{(n-k)!k!} kp^{k-1}(1-p)^{n-k}\\
&=np\sum_{k=1}^n\frac{(n-1)!}{(n-k)!k!} kp^{k-1}(1-p)^{n-k}\\
&=np\sum_{k=1}^n\frac{(n-1)!}{[(n-1)-(k-1)]!(k-1)!}p^{k-1}(1-p)^{n-k}\\
&=np\sum_{k=1}^n{n-1 \choose k-1}p^{k-1}(1-p)^{n-k}\\
&=np\sum_{k=0}^{n-1}{n-1 \choose k}p^k(1-p)^{n-1-k}\\
&=np\sum_{k=0}^{m}{m \choose k}p^k(1-p)^{m-k}\\
&=np(1-p+p)^m\\
&=np
\end{split}
\end{equation*}

How to calculate variance for \(X\stackrel{d}{=} B(n, p)\)? See \href{https://en.wikipedia.org/wiki/Binomial_distribution#Variance}{Wiki} for more
introduction.

Generally, we define a function \(A(q)=\sum_{i=0}^ni^2{n \choose i}q^i(1-p)^{n-i}\) when \(q\in [0, 1]\).

We know for \(F(q) = (1-p+q)^n\):
\begin{equation*}
    F(q) = \sum_{i=0}^n {n \choose i}q^i(1-p)^{n-i}
\end{equation*}

Further:
\begin{equation*}
    qF'(q)=\sum_{i=0}^n i{n \choose i}q^{i}(1-p)^{n-i}
\end{equation*}

Again:
\begin{equation}
    q[qF'(q)]'=\sum_{i=0}^n i^2{n \choose i}q^i(1-p)^{n-i}=A(q)
\end{equation}

Meanwhile, we know:
\begin{equation*}
    qF'(q)=qn(1-p+q)^{n-1}
\end{equation*}

And then:
\begin{equation}
    q[qF'(q)]'=qn(1-p+q)^{n-1}+q^2n(n-1)(1-p+q)^{n-2}
\end{equation}

We also know that \(A(p)=\sum_{i=0}^ni^2{n \choose i}p^i(1-p)^{n-i}=E(X^2)\).

And through (14) and (15) we know:
\begin{equation}
    A(p)=pn+p^2n(n-1)
\end{equation}

Therefore:
\begin{equation}
    \Var(X)=E(x^2)-[E(x)]^2=pn+p^2n(n-1)-(np)^2=np(1-p)
\end{equation}

\textbf{Linear Regression}

We know for \(\hat{Y}=\hat{\beta_0}+\hat{\beta_1}(X_i-\overline{X})\), through least squares:

\begin{equation}
\left\{
\begin{split}
\hat{\beta_0}&=\overline{Y}\\
\hat{\beta_1}&=\frac{\sum_{i=1}^n(X_i-\overline{X})Y_i}{\sum_{i=1}^n(X_i-\overline{X})^2}
\end{split}
\right.
\end{equation}

Let:

\begin{equation*}
    \delta_i=Y_i-\hat{Y_i}
\end{equation*}

We say:

\begin{equation*}
    \hat{\sigma^2}=\frac{1}{n-2}\sum_{i=1}^n\delta_i^2
\end{equation*}

is an unbiased estimation of error variance.

How to prove?

First:
\begin{equation}
    Y_i-\hat{Y_i}=\beta_0+\beta_1(X_i-\overline{X_i})+e_i-\hat{\beta_0}-\hat{\beta_1}(X_i-\overline{X})
\end{equation}

And:
\begin{equation}
    \begin{split}
    \beta_0-\hat{\beta_0}&=\beta_0-\overline{Y}\\
    &=\beta_0-\frac{1}{n}\sum_{i=1}^n[\beta_0+\beta_1(X_i-\overline{X}+e_i]\\
    &=\beta_0-\beta_0-\overline{e}\\
    &=-\overline{e}
    \end{split}
\end{equation}

Further:
\begin{equation}
    \begin{split}
    \beta_1-\hat{\beta_1}&=\beta_1-\frac{\sum_{i=1}^n(X_i-\overline{X})Y_i}{\sum_{i=1}^n(X_i-\overline{X})^2}\\
    &=\beta_1-\frac{\sum_{i=1}^n(X_i-\overline{X})[\beta_0+\beta_1(X_i-\overline{X})+e_i]}{\sum_{i=1}^n(X_i-\overline{X})^2}\\
    &=\frac{\beta{1}\sum_{i=1}^n(X_i-\overline{X})^2}{\sum_{i=1}^n(X_i-\overline{X})^2}-\frac{\beta_0\sum_{i=1}^n(X_i-\overline{X})}{\sum_{i=1}^n(X_i-\overline{X})^2}\\
    &-\frac{\beta{1}\sum_{i=1}^n(X_i-\overline{X})^2}{\sum_{i=1}^n(X_i-\overline{X})^2}-\frac{\sum_{i=1}^n(X_i-\overline{x})e_i}{\sum_{i=1}^n(X_i-\overline{X})^2}\\
    &=-\frac{\sum_{i=1}^n(X_i-\overline{x})e_i}{\sum_{i=1}^n(X_i-\overline{X})^2}
    \end{split}
\end{equation}

Therefore:
\begin{equation}
\delta_i=e_i-\overline{e}-\frac{\sum_{i=1}^n(X_i-\overline{x})e_i}{\sum_{i=1}^n(X_i-\overline{X})^2}
\end{equation}

Also:
\begin{equation}
\sum_{i=1}^n[\frac{\sum_{i=1}^n(X_i-\overline{x})e_i}{\sum_{i=1}^n(X_i-\overline{X})^2}]^2=
\frac{[\sum_{i=1}^n(X_i-\overline{x})e_i]^2}{\sum_{i=1}^n(X_i-\overline{X})^2}
\end{equation}

And:
\begin{equation}
\sum_{i=1}^n(e_i-\overline{e})(X_i-\overline{X})=\sum_{i=1}^n(e_i)(X_i-\overline{X})
\end{equation}

Therefore:
\begin{equation}
\sum_{i=1}^n\delta_i^2=\sum_{i=1}^n(e_i-\overline{e})^2-\frac{[\sum_{i=1}^n(X_i-\overline{x})e_i]^2}{\sum_{i=1}^n(X_i-\overline{X})^2}
\end{equation}

As we know:
\begin{equation}
E[\sum_{i=1}^n(e_i-\overline{e})^2]=(n-1)\sigma^2
\end{equation}

And as we consider $X_1, X_2, \cdots, X_n$ as known variables, \(e\stackrel{d}{=}\mathcal{N}(0, \sigma^2)\):

\begin{equation}
E[\sum_{i=1}^n(X_i-\overline{X})e_i]=\Var{[\sum_{i=1}^n(X_i-\overline{X})e_i]}=\sigma^2\sum_{i=1}^n(X_i-\overline{X})^2
\end{equation}

Finally we get:

\begin{equation}
E(\sum_{i=1}^n\delta^2)=(n-2)\sigma^2
\end{equation}

Proved.

\textbf{Probability density function for continuous variables}

Consider two random variables \(X_1, X_2\), their probability density function is \(f(x_1, y_2)\).
Meanwhile, random variables \(Y_1, Y_2\) could be presented as:
\begin{equation} 
\left\{
\begin{split}
Y_1&=g_1(X_1, X_2)\\
Y_2&=g_2(X_1, X_2)
\end{split}
\right.
\end{equation}

What is the probability density function for \((Y_1, Y_2)\), aka \(l(y_1, y_2)\)?

We assume that \(g_1, g_2\) are bijection, so that there exist invert function:

\begin{equation}
\left\{
\begin{split}
    X_1&=h_1(Y_1, Y_2)\\
    X_2&=h_2(Y_1, Y_2)
\end{split}
\right.
\end{equation}

We also assume that \(h_1, h_2\) have continuous first-order derivatives. And as \(g_1, g_2\) are
bijection (so as \(h_1, h_2\)), the jacobian matrix:

\begin{equation}
J(y_1,y_2)=\left|
\begin{array}{ccc}
\partial h_1/\partial y_1 & \partial h_1/\partial y_2 \\
\partial h_2/\partial y_1 & \partial h_2/\partial y_2
\end{array}
\right|
\end{equation}

We know that \(P((Y_1,Y_2)\in A)=P((X_1,X_2)\in B)\) where \(B\) is mapped from \(A\) by \(h_1, h_2\).
While \(P((X_1, X_2)\in B)\) could be presented by double integral:
\begin{equation}
\iint_Bf(x_1, x_2)dx_1dx_2
\end{equation}

Therefore:
\begin{equation}
\begin{split}
P((Y_1,Y_2)\in A)&=\iint_Bf(x_1, x_2)dx_1dx_2\\
&=\iint_Af(h_1(y_1,y_2),h_2(y_1,y_2))|J(y_1,y_2)|dy_1dy_2
\end{split}
\end{equation}

(jacobian matrix is used to change variables in the double integral.)
\vspace{3mm}

Back to the probability function \(F(y)=P(Y\leq y)\), of \(Y=X_1+X_2\) let:
\begin{equation}
\begin{split}
    Y_1&=X_1+X_2\\
    Y_2&=X_2
\end{split}
\end{equation}

We then know that for \((Y_1,Y_2)\in A\), the probability function \(F(Y_1,Y_2)\):

\begin{equation}
\begin{split}
F(Y_1, Y_2)&=\iint_Af(y_1-y_2, y_2)|J(y_1,y_2)|dy_1dy_2\\
&=\iint_Af(y_1-y_2,y_2)dy_1dy_2
\end{split}
\end{equation}

As we know for \(F(y)\), the area \(A\) could be presented as \((-\infty, y]\times(-\infty, +\infty)\),
therefore:

\begin{equation}
\begin{split}
F(y)&=\iint_Af(y_1-y_2,y_2)dy_1dy_2\\
&=\int_{-\infty}^{y}dy_1\int_{-\infty}^{\infty}f(y_1-y_2,y_2)dy_2
\end{split}
\end{equation}

Further:
\begin{equation}
\begin{split}
f(y)&=F'(y)=\int_{-\infty}^{\infty}f(y-y_2,y_2)dy_2\\
&=\int_{-\infty}^{\infty}f(y-x,x)dx\\
&=\int_{-\infty}^{\infty}f(x,y-x)dx
\end{split}
\end{equation}

It is the same as (4).

\textbf{t distribution}

Having iid variables \(X_1, X_2, \cdots, X_n, N\sim (\mu, \sigma)\).
Let \(\overline{X}=\frac{1}{n}\Sum{X}{i}\), then

\begin{equation}
    t=\frac{\sqrt{n}(\overline{X}-\mu)}{s}
\end{equation}
follows \(t_{n-1}\) distribution. Where:

\begin{equation}
    s=\sqrt{\frac{\sum_{i=1}^n(x_i-\overline{x})^2}{n-1}}
\end{equation}

Let \(f(x_1, x_2, \cdots, x_n)\) be the joint probability density function 
of \(X_1, X_2, \cdots, X_n\). Let \(A\) be a \(n\times n\) orthogonal matrix, which each
element of the first row is \(1/\sqrt{n}\).
Let:

\begin{equation}
    \begin{bmatrix}
        Y_1\\
        Y_2\\
        \vdots\\
        Y_n
    \end{bmatrix} = A
    \begin{bmatrix}
        X_1\\
        X_2\\
        \vdots\\
        X_n
    \end{bmatrix}
\end{equation}

The \(n\) linear transformations could be presented as functions:

\begin{equation}
    \left\{
        \begin{split}
            Y_1 &= g_1(X_1, X_2, \cdots, X_n)\\
            Y_2 &= g_2(X_1, X_2, \cdots, X_n)\\
            &\vdots \\
            Y_n &= g_n(X_1, X_2, \cdots, X_n)\\
        \end{split}
    \right.
\end{equation}

As the determinant of \(A\) is not \(0\), \(g_1, g_2, \cdots, g_n\)
are invertible (let presenting them as \(h_1, h_2, \cdots, h_n)\).

Therefore, the joint probability density function of \(Y_1, Y_2, \cdots,
Y_n\) could be presented as:

\begin{equation}
    \begin{split}
    g(y_1,y_2,\cdots,y_n)&=f(h_1(y_1,y_2,\cdots,y_n),\\
    &h_2(y_1,y_2,\cdots,y_n),\cdots,h_n(y_1,y_2,\cdots,y_n))\\
    &|J(y_1,y_2,\cdots,y_n)|
    \end{split}
\end{equation}

As \(A\) is an orthogonal matrix, \(|J(y_1,y_2,\cdots,y_n|=1\), therefore:

\begin{equation}
\begin{split}
g(y_1,y_2,\cdots,y_n)&=f(h_1(y_1,y_2,\cdots,y_n),\\
    &h_2(y_1,y_2,\cdots,y_n),\cdots,h_n(y_1,y_2,\cdots,y_n))\\
    &=(\sqrt{2\pi\sigma})^{-n}e^{-\frac{1}{2\sigma^2}(\sum_
    {i=1}^nx_i^2-2\mu\sum_{i=1}^nx_i+\mu^2)}
\end{split}
\end{equation}

As \(A\) is an orthogonal matrix:
\begin{equation}
    \sum_{i=0}^nX_i^2=\sum_{i=0}^nY_i^2
\end{equation}

Also as each element of first row is \(\sqrt{1}/{n}\):

\begin{equation}
    \sum_{i=0}^nX_i = \sqrt{n}{Y_1}
\end{equation}

So:

\begin{equation}
    \begin{split}
    &(\sqrt{2\pi\sigma})^{-n}e^{-\frac{1}{2\sigma^2}(\sum_
        {i=1}^nx_i^2-2\mu\sum_{i=1}^nx_i+\mu^2)}\\
    &=(\sqrt{2\pi\sigma})^{-n}e^{-\frac{1}{2\sigma^2}(
        \sum_{i=1}^ny_i^2-2\sqrt{n}y_1+\mu^2)}\\
    &=(\sqrt{2\pi\sigma})^{-n}e^{-\frac{1}{2\sigma^2}
    (y_1^2-2\sqrt{n}y_1+\mu^2+\sum_{i=2}^ny_i^2)}\\
    &=(\sqrt{2\pi\sigma})^{-1}e^{-\frac{1}{2\sigma^2}(y_1-\sqrt{n}\mu)^2}\\
    &\cdot(\sqrt{2\pi\sigma})^{-1}e^{-\frac{1}{2\sigma^2}y_2^2}\cdot
    \cdots(\sqrt{2\pi\sigma})^{-1}e^{-\frac{1}{2\sigma^2}y_n^2}
    \end{split}
\end{equation}

Therefore, \(y_1,y_2,\cdots,y_n\) are independent, and:

\begin{equation}
    \begin{split}
   &Y_1\sim N(\sqrt{n}\mu, \sigma) \\
   &Y_1\sim N(0, \sigma) \\
   &\vdots \\
   &Y_n\sim N(0, \sigma) \\
    \end{split}
\end{equation}

As \(Y_1=\frac{1}{\sqrt{n}}\sum_{i=1}^nX_i\),

\begin{equation}
    \frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\sim N(0,1)
\end{equation}

Given in (1):

\begin{equation}
    \sum_{i=2}^nY_i^2=\sum_{i=1}^{n}X_i^2-(\sum_{i=1}^nX_i)^2/n
    =\sum_{i=1}^n(X_i-\overline{X})^2
\end{equation}

So, \(\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\overline{X})^2\sim \chi_{n-1}\)

\vspace{3mm}

It is also proved that \(X_i\) is independent from \(\sum_{i=2}^nX_i^2\).

Therefore:

\begin{equation}
\begin{split}
&\frac{\sqrt{n}(\overline{x}-\mu)}{\sigma}/
\sqrt{\frac{1}{\sigma^2(n-1)}\sum_{i=1}^n(X_i-\overline{X})^2}\\
&=\frac{\sqrt{n}(\overline{X}-\mu)}{s}
\end{split}
\end{equation}
follows \(t_{n-1}\).

\end{document}